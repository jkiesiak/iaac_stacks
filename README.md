# Infrastructure as Code (IaC) Implementation Project - ELT Pipeline

## Table of Contents
1. [Project Goals](#project-goals)
2. [General Overview](#general-overview)
3. [Technologies](#used-technologies) 
4. [Architecture](#architecture)
    - [Diagram](#diagram)
    - [Resources Used](#resources-used)
    - [Core Functionalities](#core-functionalities)
5. [Step-by-Step Guide](#getting-started)
    - [Project structure](#project-structure)
    - [Terraform](#terraform)
    - [AWS CDK](#aws-cdk)
    - [JSON Templates (Generated by CDK)](#cdk)
    - [Prerequisites](#prerequisites)
    - [Deployment of Resources](#deployment-of-resources)
    - [Validation & Testing](#validation--testing)
6. [License](#license)

## Project Goals

This project aims to:
- Compare popular Infrastructure as Code tools by deploying the same AWS architecture using each.
- Understand trade-offs in terms of complexity, maintainability, and readability.
- Provide hands-on experience with Terraform, AWS CDK, and raw CloudFormation JSON.
- Enable repeatable infrastructure provisioning using version-controlled code.

## General overview
Infrastructure as Code (IaC) is a practice for managing and provisioning cloud resources automatically through code, 
enabling efficient, scalable, and repeatable deployments. This project aims to implement an IaC solution using various 
technologies to deploy and manage cloud resources effectively. The primary objective is to gain hands-on experience with 
different IaC tools, including Terraform and AWS CDK, and to compare their features, usability, and performance. 
By implementing the same architecture across these technologies, the project evaluates their strengths, differences, 
and suitability for managing cloud infrastructure.

## Technologies 
This repository demonstrates three distinct approaches to defining and deploying cloud infrastructure using:

- Terraform

- AWS Cloud Development Kit (CDK)

- JSON templates generated from CDK (cdk synth)

Each method is isolated in its own directory and includes dedicated scripts to manage deployment and teardown of 
resources on AWS.


## Architecture
### Diagram
The following diagram illustrates the architecture:
![Optional Image Alt Text](docs/architecture_v3.svg)

### Resources Used
The architecture consists of a serverless, event-driven system deployed on AWS, utilizing the following resources:
- **Amazon S3**: Two buckets are used—one event bucket for receiving JSON files and one backup bucket serving as 
a destination for processed data.
- **AWS Lambda:**: Executes code for processing JSON data and performing database operations, triggered by events from 
the S3 bucket or Step Functions.
- **Amazon API Gateway**: Provides RESTful endpoints to access the system (`/customers`, `/orders`, `/orders/{id}`, 
`/customers/{id}`) for querying and modifying data.
- **AWS Step Functions**: Orchestrates the workflow for processing JSON files, coordinating Lambda functions and data movement.
- **Amazon RDS**: Stores structured data in a managed relational database for the application.
- **AWS Identity and Access Management (IAM)**: Manages secure access and permissions for Lambda functions etc
- **Amazon VPC and Security Groups**: The entire infrastructure is deployed within a Virtual Private Cloud (VPC) to 
isolate resources and control network access. A custom security group is configured to restrict inbound and outbound 
traffic. The RDS instance is placed in public subnets with inbound access allowed only on port 5432. 
For external access, Amazon API Gateway is 
deployed with a public endpoint, allowing HTTP traffic on port 443 (HTTPS). This setup ensures tight network 
boundaries, protecting backend systems while enabling secure API access.


### Core Functionalities

The system is event-driven, with key functionalities implemented as follows:

1. **Database Schema Configuration**:
   - Terraform: Configures the RDS database schema using direct `psql` PostgreSQL commands, executing external sql script 
defining two tables with primary and foreign keys and creating a user with admin privileges.
   - AWS CDK: Uses an additional Lambda function to execute an external SQL script, setting up the same database schema 
with two tables and an admin user.
   - JSON: The Json is generated based on the CDK code, hence the architecture is the same and the functionalities. 

2. **Event-Driven Processing**: 
When a JSON file is uploaded to the event S3 bucket, it triggers an AWS Step Function. The Step Function orchestrates 
   a Lambda function that processes the JSON data and inserts it into the RDS database. After processing, the file is 
   moved to the backup S3 bucket for storage.

3. **Error Handling and File Management**:
If data insertion into the database fails, the file is explicitly moved to an "unprocessed" directory, ensuring 
the issue is logged for further investigation.

4. **API Gateway Endpoints**:
The API Gateway exposes four endpoints to interact with the RDS database:

GET `/customers`: Retrieves customer data.

GET `/orders`: Retrieves order data.

PUT `/orders/{id}`: Updates specific order data by ID.

PUT `/customers/{id}`: Updates specific customer data by ID.

5. **Robust Logging**:
Detailed logs capture all operations, providing developers with insights into system behavior and error causes.


## Step-by-Step Guide

The goal of this project is to implement a repeatable and deployable infrastructure as code solution that enables teams 
to rapidly provision, manage, and tear down ELT (Extract, Load, Transform) pipelines with minimal manual intervention. 
In this section, I want to focus on establishing a standardized, automated approach that eliminates the complexity and 
time overhead typically associated with data pipeline deployment.

### Project structure

```
infra_stacks/
├── .gitignore                      # Git ignore patterns
├── README.md                       # Project documentation
│
├── cdk_stack_infrastructure/       # AWS CDK infrastructure definitions
│   ├── cdk.out/                    # CDK synthesis output (dev)
│   ├── cdk.out-prod/               # CDK synthesis output (prod)
│   ├── stacks/                     # CDK stack definitions
│   ├── lambda_grant_token_access/  # Lambda function for token management
│   ├── lambda_insert_data_into_rds/ # Lambda function for RDS data insertion
│   ├── lambda_rest_api/            # Lambda function for REST API
│   ├── lambda_store_backup/        # Lambda function for backup operations
│   ├── apply_sql_schema/           # SQL schema application scripts
│   ├── dependencies/               # External dependencies and libraries
│   ├── app.py                      # CDK application entry point
│   ├── cdk.json                    # CDK configuration
│   ├── requirements.txt            # CDK Python dependencies
│   ├── naming_utils.py             # Utility functions for resource naming
│   ├── deploy_cdk.sh               # Deployment automation script
│   ├── destroy-stacks.sh           # Stack teardown automation script
│   └── README.md                   # CDK-specific documentation
│
├── terraform/                      # Terraform infrastructure modules
│   ├── builds/                     # Lambda deployment packages
│   ├── lambda/                     # Lambda function source code
│   ├── lambda_grant_token_access/  # Token access Lambda sources
│   ├── lambda_rest_api/            # REST API Lambda sources
│   ├── lambda_store_backup/        # Backup Lambda sources
│   ├── sql_schema/                 # Database schema definitions in sql script
│   ├── main.tf                     # Main Terraform configuration
│   ├── variables.tf                # Input variables
│   ├── outputs.tf                  # Output values
│   ├── network.tf                  # VPC and networking resources
│   ├── rds.tf                      # RDS database configuration
│   ├── s3.tf                       # S3 bucket configurations
│   ├── lambda.tf                   # Lambda function definitions
│   ├── lambda_authorize_token.tf   # Token authorization Lambda
│   ├── lambda_rest_api.tf          # REST API Lambda configuration
│   ├── lambda_store_backup.tf      # Backup Lambda configuration
│   ├── rest_api.tf                 # API Gateway configuration
│   ├── step_functions.tf           # Step Functions workflows
│   ├── run-terraform.sh            # Terraform execution script
│   └── delete-terraform.sh         # Infrastructure teardown script
│
├── json_infra/                     # JSON-based infrastructure configs
│   ├── MainStack-production.template.json                    # Main production stack template
│   ├── MainStackproductionApiGatewayStackproductionE17904C5.template.json  # API Gateway stack template
│   ├── MainStackproductionLambdaRdsStackproductionDAF57C66.template.json   # Lambda & RDS stack template
│   ├── MainStackproductionRDSStackproduction3C5AE54B.template.json         # RDS database stack template
│   ├── MainStackproductionVpcStackproductionCBA518B0.template.json         # VPC networking stack template
│   ├── deploy.sh                   # JSON-based deployment script
│   ├── delete.sh                   # JSON-based teardown script
│   └── README.md                   # JSON infrastructure documentation
│
├── input_test_data/                # Sample data for testing
│
└── docs/                           # Architecture diagrams
```


### Terraform
Terraform is an open-source Infrastructure as Code (IaC) tool developed by HashiCorp. It allows you to define cloud and 
on-prem resources using a declarative language (HCL), and it manages their lifecycle with a state-based approach.

- Directory: `/terraform`

- Deployment Script: `deploy.sh`

The script accepts two mandatory command-line arguments:

`-p, --profile` : Specifies the AWS profile to be used for AWS authentication. 

`-w, --workspace`: Defines the Terraform workspace to be used or created for managing resources.

```bash
./run-terraform.sh -p <aws_profile> -w <workspace_name>
./run-terraform.sh -p user_infra -w vol9
```

- Deletion Script: `delete.sh`

The delete-terraform.sh script automates the cleanup process. This script ensures that all provisioned resources are 
safely and efficiently destroyed, avoiding any manual intervention. 
It helps maintain a clean environment by removing associated resources and preventing unintended costs.

```bash
./delete-terraform.sh
```

Safety precautions:
To avoid accidental deletion of critical resources, the script requires manual updated to specific parameters before execution.
Please, update following values in a script:
- `aws_profile` : The AWS profile used for authentication 
- `workspace` : Defines the Terraform workspace associated to the provisioned resources


### AWS CDK
The AWS Cloud Development Kit (CDK) enables you to define cloud infrastructure using familiar programming languages 
like TypeScript, Python, and Java. It synthesizes these constructs into CloudFormation templates.

- Directory: `/cdk_stack_infrastructure`

- Deployment Script: `deploy.sh`

- Deletion Script: `delete.sh`

### JSON Templates (Generated by CDK)
CDK can also be used to synthesize infrastructure into raw JSON CloudFormation templates without deploying them. This 
approach can be useful for inspecting or versioning your IaC definitions.
The JSON directory contains synthesized CloudFormation templates using `cdk synth`, enabling review and deployment via 
raw AWS CLI or scripted methods without executing CDK directly

- Directory: `/json_infra`

- Deployment Script: `deploy.sh`

- Deletion Script: `delete.sh`


### Prerequisites
Before running the automation scripts or deploying the infrastructure, ensure you have AWS CLI installed and configured 
on your system.
If not, you can configure it using following command: ```aws configure --profile <your-profile>```

## Validation & Testing

After deployment:

- Use the API Gateway endpoints to test connectivity and data flow.
- Upload sample JSON files to the S3 bucket and verify Step Function executions and data population in RDS.
- Inspect logs in CloudWatch to ensure proper error handling and execution tracing.
Test deployed serverless application using files available in the directory

- `input_test_data/` - Sample datasets and validation schemas for pipeline testing and development

## Future Improvements

- Integrate GitHub Actions or AWS CodePipeline for automated deployments.
- Implement unit/integration testing for Lambda functions.


### License 
This project is the intellectual property of the author and is not licensed for redistribution, modification, or 
commercial use without explicit written permission.

© 2025 [Joanna Kiesiak]. All rights reserved.
